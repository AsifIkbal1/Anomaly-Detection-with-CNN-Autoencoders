{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this Jupyter notebook, we will embark on an exploration of the intriguing field of **electrocardiogram (ECG)** signal processing and classification, utilizing the capabilities of a **Convolutional Neural Network (CNN) Autoencoder**. Our chosen dataset for this journey is the PTB Diagnostic ECG Database, a meticulously curated collection of ECG signals crafted explicitly for diagnostic purposes. Our primary objective is to construct a robust Autoencoder model tailored for the task of detecting anomalous electrocardiogram (ECG) signals.","metadata":{}},{"cell_type":"markdown","source":"## Dataset Overview\n\nThe **PTB Diagnostic ECG Database** is a collection of 14,552 ECG recordings sourced from Physionet's PTB Diagnostic Database. These ECG signals are categorized into two classes: normal heartbeats and those affected by cardiac abnormalities. The dataset is sampled at 125Hz, providing high-resolution data for in-depth analysis.\n\n> Let's delve into some essential details about the PTB Diagnostic ECG Database:\n> \n> - **Number of Samples:** 14,552\n> - **Number of Categories:** 2\n> - **Sampling Frequency:** 125Hz\n> - **Data Source:** Physionet's PTB Diagnostic Database","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color:red\">We kindly request that if you find this notebook useful, please consider upvoting it as a token of appreciation.</span>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnp.set_printoptions(suppress=True)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.io import arff\nfrom sklearn.model_selection import train_test_split\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (6, 4)\nplt.style.use(\"ggplot\")\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import mae\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score, classification_report\nimport os","metadata":{"id":"CLQ5iE5dArIX","execution":{"iopub.status.busy":"2023-09-10T06:04:30.136288Z","iopub.execute_input":"2023-09-10T06:04:30.137021Z","iopub.status.idle":"2023-09-10T06:04:30.14893Z","shell.execute_reply.started":"2023-09-10T06:04:30.136975Z","shell.execute_reply":"2023-09-10T06:04:30.147772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\ngpus","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:30.152247Z","iopub.execute_input":"2023-09-10T06:04:30.153005Z","iopub.status.idle":"2023-09-10T06:04:30.164805Z","shell.execute_reply.started":"2023-09-10T06:04:30.15297Z","shell.execute_reply":"2023-09-10T06:04:30.163724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:30.168333Z","iopub.execute_input":"2023-09-10T06:04:30.172058Z","iopub.status.idle":"2023-09-10T06:04:30.18087Z","shell.execute_reply.started":"2023-09-10T06:04:30.172026Z","shell.execute_reply":"2023-09-10T06:04:30.179902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_df = pd.read_csv(\"/kaggle/input/heartbeat/ptbdb_normal.csv\")\nanomaly_df = pd.read_csv(\"/kaggle/input/heartbeat/ptbdb_abnormal.csv\")\nnormal_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:30.188119Z","iopub.execute_input":"2023-09-10T06:04:30.188894Z","iopub.status.idle":"2023-09-10T06:04:31.135468Z","shell.execute_reply.started":"2023-09-10T06:04:30.188843Z","shell.execute_reply":"2023-09-10T06:04:31.134515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(\"Shape of Normal data\", normal_df.shape)\nprint(\"Shape of Abnormal data\", anomaly_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.139555Z","iopub.execute_input":"2023-09-10T06:04:31.139888Z","iopub.status.idle":"2023-09-10T06:04:31.146994Z","shell.execute_reply.started":"2023-09-10T06:04:31.139835Z","shell.execute_reply":"2023-09-10T06:04:31.145913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below generates a visual comparison between two randomly selected ECG signal samples: one from the \"Normal\" dataset and the other from the \"Anomaly\" dataset. This side-by-side plot enables a direct visual assessment of ECG signal patterns between normal and anomalous cases.","metadata":{}},{"cell_type":"code","source":"def plot_sample(normal, anomaly):\n    index = np.random.randint(0, len(normal_df), 2)\n    \n    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n    ax[0].plot(normal.iloc[index[0], :].values, label=f\"Case {index[0]}\")\n    ax[0].plot(normal.iloc[index[1], :].values, label=f\"Case {index[1]}\")\n    ax[0].legend(shadow=True, frameon=True, facecolor=\"inherit\", loc=1, fontsize=9)\n    ax[0].set_title(\"Normal\")\n    \n    ax[1].plot(anomaly.iloc[index[0], :].values, label=f\"Case {index[0]}\")\n    ax[1].plot(anomaly.iloc[index[1], :].values, label=f\"Case {index[1]}\")\n    ax[1].legend(shadow=True, frameon=True, facecolor=\"inherit\", loc=1, fontsize=9)\n    ax[1].set_title(\"Anomaly\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.148329Z","iopub.execute_input":"2023-09-10T06:04:31.149429Z","iopub.status.idle":"2023-09-10T06:04:31.160206Z","shell.execute_reply.started":"2023-09-10T06:04:31.149392Z","shell.execute_reply":"2023-09-10T06:04:31.159016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_sample(normal_df, anomaly_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.165096Z","iopub.execute_input":"2023-09-10T06:04:31.165408Z","iopub.status.idle":"2023-09-10T06:04:31.731544Z","shell.execute_reply.started":"2023-09-10T06:04:31.165373Z","shell.execute_reply":"2023-09-10T06:04:31.730622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASS_NAMES = [\"Normal\", \"Anomaly\"]\n\nnormal_df_copy = normal_df.copy()\nanomaly_df_copy = anomaly_df.copy()\nprint(anomaly_df_copy.columns.equals(normal_df_copy.columns))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.732656Z","iopub.execute_input":"2023-09-10T06:04:31.733231Z","iopub.status.idle":"2023-09-10T06:04:31.746201Z","shell.execute_reply.started":"2023-09-10T06:04:31.733205Z","shell.execute_reply":"2023-09-10T06:04:31.745225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_df_copy = normal_df_copy.set_axis(range(1, 189), axis=1)\nanomaly_df_copy = anomaly_df_copy.set_axis(range(1, 189), axis=1)\nnormal_df_copy = normal_df_copy.assign(target = CLASS_NAMES[0])\nanomaly_df_copy = anomaly_df_copy.assign(target = CLASS_NAMES[1])\n\n\ndf = pd.concat((normal_df_copy, anomaly_df_copy))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.747583Z","iopub.execute_input":"2023-09-10T06:04:31.747947Z","iopub.status.idle":"2023-09-10T06:04:31.772209Z","shell.execute_reply.started":"2023-09-10T06:04:31.747916Z","shell.execute_reply":"2023-09-10T06:04:31.771299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Smoothed Mean Plot for Class Comparison\n\nIn this Python code, a function called `plot_smoothed_mean` is defined for creating smoothed mean plots. It takes input data, a class name, and a step size as parameters. The function calculates the rolling mean and standard deviation of the data with the specified step size, then plots the smoothed mean along with a shaded area representing the margin of three times the standard deviation.\n\nThe code also demonstrates the use of this function to compare the smoothed means of different classes. It creates a subplot with two panels, each representing a different class. The data for each class is grouped, and the mean is computed before passing it to the `plot_smoothed_mean` function. This allows for visualizing how the smoothed means of different classes compare.\n\nThe resulting plot provides insights into how the means of different classes vary over time or some other variable represented by the data, with shaded areas indicating the uncertainty around the mean estimates.\n","metadata":{}},{"cell_type":"code","source":"def plot_smoothed_mean(data, class_name = \"normal\", step_size=5, ax=None):\n    df = pd.DataFrame(data)\n    roll_df = df.rolling(step_size)\n    smoothed_mean = roll_df.mean().dropna().reset_index(drop=True)\n    smoothed_std = roll_df.std().dropna().reset_index(drop=True)\n    margin = 3*smoothed_std\n    lower_bound = (smoothed_mean - margin).values.flatten()\n    upper_bound = (smoothed_mean + margin).values.flatten()\n\n    ax.plot(smoothed_mean.index, smoothed_mean)\n    ax.fill_between(smoothed_mean.index, lower_bound, y2=upper_bound, alpha=0.3, color=\"red\")\n    ax.set_title(class_name, fontsize=9)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:31.773677Z","iopub.execute_input":"2023-09-10T06:04:31.774048Z","iopub.status.idle":"2023-09-10T06:04:31.78161Z","shell.execute_reply.started":"2023-09-10T06:04:31.774017Z","shell.execute_reply":"2023-09-10T06:04:31.780577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\naxes = axes.flatten()\nfor i, label in enumerate(CLASS_NAMES, start=1):\n    data_group = df.groupby(\"target\")\n    data = data_group.get_group(label).mean(axis=0, numeric_only=True).to_numpy()\n    plot_smoothed_mean(data, class_name=label, step_size=20, ax=axes[i-1])\nfig.suptitle(\"Plot of smoothed mean for each class\", y=0.95, weight=\"bold\")\nplt.tight_layout()","metadata":{"id":"0duZ5mVFNxnK","outputId":"e0bb53f7-096e-4ca7-d0fe-42d12a9c9ba7","execution":{"iopub.status.busy":"2023-09-10T06:04:31.784372Z","iopub.execute_input":"2023-09-10T06:04:31.78466Z","iopub.status.idle":"2023-09-10T06:04:32.401422Z","shell.execute_reply.started":"2023-09-10T06:04:31.784634Z","shell.execute_reply":"2023-09-10T06:04:32.399898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data to training and testing set","metadata":{}},{"cell_type":"code","source":"normal_df.drop(\"target\", axis=1, errors=\"ignore\", inplace=True)\nnormal = normal_df.to_numpy()\nanomaly_df.drop(\"target\", axis=1, errors=\"ignore\", inplace=True)\nanomaly = anomaly_df.to_numpy()\n\nX_train, X_test = train_test_split(normal, test_size=0.15, random_state=45, shuffle=True)\nprint(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}, anomaly shape: {anomaly.shape}\")","metadata":{"id":"web-cxU5lH1a","outputId":"80d0b380-07ca-4400-9dc6-c07e9f579473","execution":{"iopub.status.busy":"2023-09-10T06:04:32.403008Z","iopub.execute_input":"2023-09-10T06:04:32.40366Z","iopub.status.idle":"2023-09-10T06:04:32.422054Z","shell.execute_reply.started":"2023-09-10T06:04:32.403624Z","shell.execute_reply":"2023-09-10T06:04:32.420487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building CNN Autoencoder Model","metadata":{}},{"cell_type":"markdown","source":"## About Autoencoders: Learning Efficient Data Representations\n\nAutoencoders are a class of neural network architectures commonly used in unsupervised machine learning and deep learning tasks. Their primary purpose is to discover and learn efficient representations of data by encoding it into a lower-dimensional latent space and subsequently decoding it back to its original form. Autoencoders play a crucial role in various applications, such as dimensionality reduction, data denoising, anomaly detection, and generative modeling.\n\nThe core components of an autoencoder consist of an encoder and a decoder. The encoder maps input data to the latent space, while the decoder reconstructs the data from its encoded representation. During training, autoencoders aim to minimize the reconstruction error between the input and the decoded output, which results in the learning of meaningful data representations.\n\nAutoencoders offer a versatile tool for feature extraction, data compression, and more, making them a valuable addition to the toolkit of data scientists and machine learning practitioners.\n","metadata":{}},{"cell_type":"markdown","source":"![Autoencoder](https://miro.medium.com/v2/resize:fit:640/format:webp/1*nqzWupxC60iAH2dYrFT78Q.png)","metadata":{}},{"cell_type":"code","source":"class AutoEncoder(Model):\n    def __init__(self, input_dim, latent_dim):\n        super(AutoEncoder, self).__init__()\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n\n        self.encoder = tf.keras.Sequential([\n            layers.Input(shape=(input_dim,)),\n            layers.Reshape((input_dim, 1)),  # Reshape to 3D for Conv1D\n            layers.Conv1D(64, 3, strides=1, activation='relu', padding=\"same\"),\n            layers.BatchNormalization(),\n            layers.MaxPooling1D(2, padding=\"same\"),\n            layers.Conv1D(latent_dim, 3, strides=1, activation='relu', padding=\"same\"),\n            layers.BatchNormalization(),\n            layers.MaxPooling1D(2, padding=\"same\"),\n        ])\n\n        self.decoder = tf.keras.Sequential([\n            layers.Conv1D(latent_dim, 3, strides=1, activation='relu', padding=\"same\"),\n            layers.UpSampling1D(2),\n            layers.BatchNormalization(),\n            layers.Conv1D(64, 3, strides=1, activation='relu', padding=\"same\"),\n            layers.UpSampling1D(2),\n            layers.BatchNormalization(),\n            layers.Flatten(),\n            layers.Dense(input_dim)\n        ])\n\n    def call(self, X):\n        encoded = self.encoder(X)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ninput_dim = X_train.shape[-1]\nlatent_dim = 32\n\nmodel = AutoEncoder(input_dim, latent_dim)\nmodel.build((None, input_dim))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=\"mae\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:04:32.426529Z","iopub.execute_input":"2023-09-10T06:04:32.426807Z","iopub.status.idle":"2023-09-10T06:04:32.720793Z","shell.execute_reply.started":"2023-09-10T06:04:32.426784Z","shell.execute_reply":"2023-09-10T06:04:32.719897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nbatch_size = 64\nearly_stopping = EarlyStopping(patience=10, min_delta=1e-3, monitor=\"val_loss\", restore_best_weights=True)\n\n\nhistory = model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size,\n                    validation_split=0.1, callbacks=[early_stopping])","metadata":{"id":"4xfyy_l5veYr","outputId":"9728b1bc-5454-4e41-c53a-90061688b3a4","execution":{"iopub.status.busy":"2023-09-10T06:04:32.722098Z","iopub.execute_input":"2023-09-10T06:04:32.722433Z","iopub.status.idle":"2023-09-10T06:04:57.513662Z","shell.execute_reply.started":"2023-09-10T06:04:32.7224Z","shell.execute_reply":"2023-09-10T06:04:57.512742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'], label=\"Training loss\")\nplt.plot(history.history['val_loss'], label=\"Validation loss\", ls=\"--\")\nplt.legend(shadow=True, frameon=True, facecolor=\"inherit\", loc=\"best\", fontsize=9)\nplt.title(\"Training loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"bfqckqK3IlXo","outputId":"9d0ec776-8226-4b31-85f9-7a4248478d90","execution":{"iopub.status.busy":"2023-09-10T06:04:57.51678Z","iopub.execute_input":"2023-09-10T06:04:57.517099Z","iopub.status.idle":"2023-09-10T06:04:57.808649Z","shell.execute_reply.started":"2023-09-10T06:04:57.517073Z","shell.execute_reply":"2023-09-10T06:04:57.807742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mae = model.evaluate(X_train, X_train, verbose=0)\ntest_mae = model.evaluate(X_test, X_test, verbose=0)\nanomaly_mae = model.evaluate(anomaly_df, anomaly_df, verbose=0)\n\nprint(\"Training dataset error: \", train_mae)\nprint(\"Testing dataset error: \", test_mae)\nprint(\"Anormaly dataset error: \", anomaly_mae)","metadata":{"id":"uM-FhXpR1YAV","outputId":"6c798066-1d41-47b2-eb83-158b0444e692","execution":{"iopub.status.busy":"2023-09-10T06:04:57.810069Z","iopub.execute_input":"2023-09-10T06:04:57.810816Z","iopub.status.idle":"2023-09-10T06:04:59.603543Z","shell.execute_reply.started":"2023-09-10T06:04:57.810781Z","shell.execute_reply":"2023-09-10T06:04:59.602521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, X):\n    pred = model.predict(X, verbose=False)\n    loss = mae(pred, X)\n    return pred, loss","metadata":{"id":"dL7N-P3mk8bL","execution":{"iopub.status.busy":"2023-09-10T06:04:59.60477Z","iopub.execute_input":"2023-09-10T06:04:59.605162Z","iopub.status.idle":"2023-09-10T06:04:59.61031Z","shell.execute_reply.started":"2023-09-10T06:04:59.605128Z","shell.execute_reply":"2023-09-10T06:04:59.609348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, train_loss = predict(model, X_train)\n_, test_loss = predict(model, X_test)\n_, anomaly_loss = predict(model, anomaly)\nthreshold = np.mean(train_loss) + 2*np.std(train_loss) # Setting threshold for distinguish normal data from anomalous data\n\nbins = 40\nplt.figure(figsize=(9, 5), dpi=100)\nsns.histplot(train_loss, bins=bins, kde=True, label=\"Train Normal\")\nsns.histplot(test_loss, bins=bins, kde=True, label=\"Test Normal\")\nsns.histplot(anomaly_loss, bins=bins, kde=True, label=\"anomaly\")\n\nax = plt.gca()  # Get the current Axes\nylim = ax.get_ylim()\nplt.vlines(threshold, 0, ylim[-1], color=\"k\", ls=\"--\")\nplt.annotate(f\"Threshold: {threshold:.3f}\", xy=(threshold, ylim[-1]), xytext=(threshold+0.009, ylim[-1]),\n             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=9)\nplt.legend(shadow=True, frameon=True, facecolor=\"inherit\", loc=\"best\", fontsize=9)\nplt.show()","metadata":{"id":"Y-KHAFZwYoQS","outputId":"2b51bef5-5237-41bb-9fce-6e7b879627b5","execution":{"iopub.status.busy":"2023-09-10T06:05:34.570701Z","iopub.execute_input":"2023-09-10T06:05:34.571447Z","iopub.status.idle":"2023-09-10T06:05:37.112675Z","shell.execute_reply.started":"2023-09-10T06:05:34.571411Z","shell.execute_reply":"2023-09-10T06:05:37.111782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_examples(model, data, ax, title):\n    pred, loss = predict(model, data)\n    ax.plot(data.flatten(), label=\"Actual\")\n    ax.plot(pred[0], label = \"Predicted\")\n    ax.fill_between(range(1, 189), data.flatten(), pred[0], alpha=0.3, color=\"r\")\n    ax.legend(shadow=True, frameon=True,\n              facecolor=\"inherit\", loc=1, fontsize=7)\n#                bbox_to_anchor = (0, 0, 0.8, 0.25))\n\n    ax.set_title(f\"{title} (loss: {loss[0]:.3f})\", fontsize=9.5)","metadata":{"id":"2O3Nto_0Y2Il","execution":{"iopub.status.busy":"2023-09-10T06:05:37.114667Z","iopub.execute_input":"2023-09-10T06:05:37.116109Z","iopub.status.idle":"2023-09-10T06:05:37.12307Z","shell.execute_reply.started":"2023-09-10T06:05:37.116074Z","shell.execute_reply":"2023-09-10T06:05:37.121977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 5, sharey=True, sharex=True, figsize=(12, 6))\nrandom_indexes = np.random.randint(0, len(X_train), size=5)\n\nfor i, idx in enumerate(random_indexes):\n    data = X_train[[idx]]\n    plot_examples(model, data, ax=axes[0, i], title=\"Normal\")\n\nfor i, idx in enumerate(random_indexes):\n    data = anomaly[[idx]]\n    plot_examples(model, data, ax=axes[1, i], title=\"anomaly\")\n\nplt.tight_layout()\nfig.suptitle(\"Sample plots (Actual vs Reconstructed by the CNN autoencoder)\", y=1.04, weight=\"bold\")\nfig.savefig(\"autoencoder.png\")\nplt.show()","metadata":{"id":"OhM07LBFvbdn","outputId":"4bd16974-154e-46b0-8024-76229c5cb978","execution":{"iopub.status.busy":"2023-09-10T06:05:37.124364Z","iopub.execute_input":"2023-09-10T06:05:37.125398Z","iopub.status.idle":"2023-09-10T06:05:40.485697Z","shell.execute_reply.started":"2023-09-10T06:05:37.125365Z","shell.execute_reply":"2023-09-10T06:05:40.484764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{"id":"lroS8YG2H99t"}},{"cell_type":"code","source":"def evaluate_model(model, data):\n    pred, loss = predict(model, data)\n    if id(data) == id(anomaly):\n        accuracy = np.sum(loss > threshold)/len(data)\n    else:\n        accuracy = np.sum(loss <= threshold)/len(data)\n    return f\"Accuracy: {accuracy:.2%}\"","metadata":{"id":"rafC-MlJ1ww4","execution":{"iopub.status.busy":"2023-09-10T06:05:40.487822Z","iopub.execute_input":"2023-09-10T06:05:40.488453Z","iopub.status.idle":"2023-09-10T06:05:40.49476Z","shell.execute_reply.started":"2023-09-10T06:05:40.48842Z","shell.execute_reply":"2023-09-10T06:05:40.493478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training\", evaluate_model(model, X_train))\nprint(\"Testing\", evaluate_model(model, X_test))\nprint(\"Anomaly\", evaluate_model(model, anomaly))","metadata":{"id":"-C47TEWZ_UXW","outputId":"0fc7cf1d-61ac-4299-bd03-e53817a2f82c","execution":{"iopub.status.busy":"2023-09-10T06:05:40.49637Z","iopub.execute_input":"2023-09-10T06:05:40.496754Z","iopub.status.idle":"2023-09-10T06:05:41.90805Z","shell.execute_reply.started":"2023-09-10T06:05:40.496722Z","shell.execute_reply":"2023-09-10T06:05:41.906997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_labels(model, train, test, anomaly, threshold=threshold):\n    ytrue = np.concatenate((np.ones(len(X_train)+len(X_test), dtype=int), np.zeros(len(anomaly), dtype=int)))\n    _, train_loss = predict(model, train)\n    _, test_loss = predict(model, test)\n    _, anomaly_loss = predict(model, anomaly)\n    train_pred = (train_loss <= threshold).numpy().astype(int)\n    test_pred = (test_loss <= threshold).numpy().astype(int)\n    anomaly_pred = (anomaly_loss < threshold).numpy().astype(int)\n    ypred = np.concatenate((train_pred, test_pred, anomaly_pred))\n    \n    return ytrue, ypred","metadata":{"execution":{"iopub.status.busy":"2023-09-10T06:05:45.169834Z","iopub.execute_input":"2023-09-10T06:05:45.170214Z","iopub.status.idle":"2023-09-10T06:05:45.178044Z","shell.execute_reply.started":"2023-09-10T06:05:45.170186Z","shell.execute_reply":"2023-09-10T06:05:45.176922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(model, train, test, anomaly, threshold=threshold):\n    ytrue, ypred = prepare_labels(model, train, test, anomaly, threshold=threshold)\n    accuracy = accuracy_score(ytrue, ypred)\n    precision = precision_score(ytrue, ypred)\n    recall = recall_score(ytrue, ypred)\n    f1 = f1_score(ytrue, ypred)\n    print(f\"\"\"\\\n        Accuracy: {accuracy:.2%}\n        Precision: {precision:.2%}\n        Recall: {recall:.2%}\n        f1: {f1:.2%}\\n\n        \"\"\")\n\n    cm = confusion_matrix(ytrue, ypred)\n    cm_norm = confusion_matrix(ytrue, ypred, normalize=\"true\")\n    data = np.array([f\"{count}\\n({pct:.2%})\" for count, pct in zip(cm.ravel(), cm_norm.ravel())]).reshape(cm.shape)\n    labels = [\"Anomaly\", \"Normal\"]\n\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=data, fmt=\"\", xticklabels=labels, yticklabels=labels)\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.title(\"Confusion Matrix\", weight=\"bold\")\n    plt.tight_layout()","metadata":{"id":"yznLPafnCPd5","execution":{"iopub.status.busy":"2023-09-10T06:05:46.064151Z","iopub.execute_input":"2023-09-10T06:05:46.065379Z","iopub.status.idle":"2023-09-10T06:05:46.07437Z","shell.execute_reply.started":"2023-09-10T06:05:46.065338Z","shell.execute_reply":"2023-09-10T06:05:46.073055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(model, X_train, X_test, anomaly, threshold=threshold)","metadata":{"id":"LlBGuJUqJLXw","outputId":"43dc2732-54ef-49ef-8b2b-7d0a8a33bbff","execution":{"iopub.status.busy":"2023-09-10T06:05:46.57919Z","iopub.execute_input":"2023-09-10T06:05:46.579497Z","iopub.status.idle":"2023-09-10T06:05:48.752471Z","shell.execute_reply.started":"2023-09-10T06:05:46.579471Z","shell.execute_reply":"2023-09-10T06:05:48.751356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation Metrics\n\nThe following evaluation metrics provide a comprehensive assessment of the performance of our model:\n\n- **Accuracy (99.15%)**: Accuracy measures the proportion of correct predictions out of all predictions made by the model, indicating an exceptionally high overall classification correctness.\n\n- **Precision (99.75%)**: Precision is the percentage of true positive predictions relative to all positive predictions made by the model, signifying the model's precision in correctly identifying positive instances.\n\n- **Recall (97.21%)**: Recall, also known as sensitivity, represents the model's ability to accurately identify positive instances out of all actual positive instances, demonstrating a strong capability to capture true positives.\n\n- **F1 Score (98.46%)**: The F1 score is a composite metric that balances precision and recall, providing an overall measure of model performance. With an F1 score close to 100%, the model effectively balances precision and recall for this task.\n\nThese metrics collectively indicate a high-performing model with a remarkable ability to make accurate predictions across the target classes.","metadata":{}},{"cell_type":"code","source":"ytrue, ypred = prepare_labels(model, X_train, X_test, anomaly, threshold=threshold)\nprint(classification_report(ytrue, ypred, target_names=CLASS_NAMES))","metadata":{"id":"Z1Atf3DbTxCx","execution":{"iopub.status.busy":"2023-09-10T06:05:53.852526Z","iopub.execute_input":"2023-09-10T06:05:53.852905Z","iopub.status.idle":"2023-09-10T06:05:55.304987Z","shell.execute_reply.started":"2023-09-10T06:05:53.852868Z","shell.execute_reply":"2023-09-10T06:05:55.303922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for taking the time to explore my notebook and delve into the world of data science with me. Your interest and engagement are greatly appreciated, and I hope you found valuable insights and inspiration within these pages.","metadata":{}}]}